{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-02T15:58:40.281422Z",
     "start_time": "2024-07-02T15:58:36.753878Z"
    }
   },
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from rdflib import Graph"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:59:06.029478Z",
     "start_time": "2024-07-02T15:58:40.300943Z"
    }
   },
   "cell_type": "code",
   "source": "llm = LLM(model=\"meta-llama/Meta-Llama-3-8B-Instruct\", dtype=\"float16\", max_num_batched_tokens=50_000, gpu_memory_utilization=0.01)",
   "id": "45cdf3b040dd3032",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-02 17:58:40 config.py:1218] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 07-02 17:58:40 llm_engine.py:161] Initializing an LLM engine (v0.5.0) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projets/melodi/gsantoss/miniconda3/envs/myenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-02 17:58:41 selector.py:119] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 07-02 17:58:41 selector.py:50] Using XFormers backend.\n",
      "INFO 07-02 17:58:42 selector.py:119] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 07-02 17:58:42 selector.py:50] Using XFormers backend.\n",
      "INFO 07-02 17:58:43 weight_utils.py:218] Using model weights format ['*.safetensors']\n",
      "INFO 07-02 17:58:55 model_runner.py:159] Loading model weights took 14.9595 GB\n",
      "INFO 07-02 17:59:05 gpu_executor.py:83] # GPU blocks: 0, # CPU blocks: 2048\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m llm \u001B[38;5;241m=\u001B[39m \u001B[43mLLM\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmeta-llama/Meta-Llama-3-8B-Instruct\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfloat16\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_num_batched_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50_000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgpu_memory_utilization\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/projets/melodi/gsantoss/miniconda3/envs/myenv/lib/python3.11/site-packages/vllm/entrypoints/llm.py:144\u001B[0m, in \u001B[0;36mLLM.__init__\u001B[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, **kwargs)\u001B[0m\n\u001B[1;32m    123\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdisable_log_stats\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    124\u001B[0m engine_args \u001B[38;5;241m=\u001B[39m EngineArgs(\n\u001B[1;32m    125\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m    126\u001B[0m     tokenizer\u001B[38;5;241m=\u001B[39mtokenizer,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    142\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    143\u001B[0m )\n\u001B[0;32m--> 144\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_engine \u001B[38;5;241m=\u001B[39m \u001B[43mLLMEngine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_engine_args\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    145\u001B[0m \u001B[43m    \u001B[49m\u001B[43mengine_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43musage_context\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mUsageContext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mLLM_CLASS\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    146\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest_counter \u001B[38;5;241m=\u001B[39m Counter()\n",
      "File \u001B[0;32m/projets/melodi/gsantoss/miniconda3/envs/myenv/lib/python3.11/site-packages/vllm/engine/llm_engine.py:360\u001B[0m, in \u001B[0;36mLLMEngine.from_engine_args\u001B[0;34m(cls, engine_args, usage_context)\u001B[0m\n\u001B[1;32m    357\u001B[0m     executor_class \u001B[38;5;241m=\u001B[39m GPUExecutor\n\u001B[1;32m    359\u001B[0m \u001B[38;5;66;03m# Create the LLM engine.\u001B[39;00m\n\u001B[0;32m--> 360\u001B[0m engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    361\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mengine_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    362\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexecutor_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexecutor_class\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    363\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlog_stats\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mengine_args\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdisable_log_stats\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    364\u001B[0m \u001B[43m    \u001B[49m\u001B[43musage_context\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43musage_context\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    365\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    366\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m engine\n",
      "File \u001B[0;32m/projets/melodi/gsantoss/miniconda3/envs/myenv/lib/python3.11/site-packages/vllm/engine/llm_engine.py:236\u001B[0m, in \u001B[0;36mLLMEngine.__init__\u001B[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, vision_language_config, speculative_config, decoding_config, executor_class, log_stats, usage_context)\u001B[0m\n\u001B[1;32m    223\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_executor \u001B[38;5;241m=\u001B[39m executor_class(\n\u001B[1;32m    224\u001B[0m     model_config\u001B[38;5;241m=\u001B[39mmodel_config,\n\u001B[1;32m    225\u001B[0m     cache_config\u001B[38;5;241m=\u001B[39mcache_config,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    232\u001B[0m     load_config\u001B[38;5;241m=\u001B[39mload_config,\n\u001B[1;32m    233\u001B[0m )\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel_config\u001B[38;5;241m.\u001B[39membedding_mode:\n\u001B[0;32m--> 236\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_initialize_kv_caches\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    238\u001B[0m \u001B[38;5;66;03m# If usage stat is enabled, collect relevant info.\u001B[39;00m\n\u001B[1;32m    239\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_usage_stats_enabled():\n",
      "File \u001B[0;32m/projets/melodi/gsantoss/miniconda3/envs/myenv/lib/python3.11/site-packages/vllm/engine/llm_engine.py:326\u001B[0m, in \u001B[0;36mLLMEngine._initialize_kv_caches\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    323\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache_config\u001B[38;5;241m.\u001B[39mnum_gpu_blocks \u001B[38;5;241m=\u001B[39m num_gpu_blocks\n\u001B[1;32m    324\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache_config\u001B[38;5;241m.\u001B[39mnum_cpu_blocks \u001B[38;5;241m=\u001B[39m num_cpu_blocks\n\u001B[0;32m--> 326\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_executor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minitialize_cache\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_gpu_blocks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_cpu_blocks\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/projets/melodi/gsantoss/miniconda3/envs/myenv/lib/python3.11/site-packages/vllm/executor/gpu_executor.py:86\u001B[0m, in \u001B[0;36mGPUExecutor.initialize_cache\u001B[0;34m(self, num_gpu_blocks, num_cpu_blocks)\u001B[0m\n\u001B[1;32m     80\u001B[0m \u001B[38;5;66;03m# NOTE: This is logged in the executor because there can be >1 worker\u001B[39;00m\n\u001B[1;32m     81\u001B[0m \u001B[38;5;66;03m# with other executors. We could log in the engine level, but work\u001B[39;00m\n\u001B[1;32m     82\u001B[0m \u001B[38;5;66;03m# remains to abstract away the device for non-GPU configurations.\u001B[39;00m\n\u001B[1;32m     83\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m# GPU blocks: \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m, # CPU blocks: \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, num_gpu_blocks,\n\u001B[1;32m     84\u001B[0m             num_cpu_blocks)\n\u001B[0;32m---> 86\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdriver_worker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minitialize_cache\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_gpu_blocks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_cpu_blocks\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/projets/melodi/gsantoss/miniconda3/envs/myenv/lib/python3.11/site-packages/vllm/worker/worker.py:187\u001B[0m, in \u001B[0;36mWorker.initialize_cache\u001B[0;34m(self, num_gpu_blocks, num_cpu_blocks)\u001B[0m\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minitialize_cache\u001B[39m(\u001B[38;5;28mself\u001B[39m, num_gpu_blocks: \u001B[38;5;28mint\u001B[39m,\n\u001B[1;32m    182\u001B[0m                      num_cpu_blocks: \u001B[38;5;28mint\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    183\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Allocate GPU and CPU KV cache with the specified number of blocks.\u001B[39;00m\n\u001B[1;32m    184\u001B[0m \n\u001B[1;32m    185\u001B[0m \u001B[38;5;124;03m    This also warms up the model, which may record CUDA graphs.\u001B[39;00m\n\u001B[1;32m    186\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 187\u001B[0m     \u001B[43mraise_if_cache_size_invalid\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnum_gpu_blocks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    188\u001B[0m \u001B[43m                                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcache_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mblock_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    189\u001B[0m \u001B[43m                                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel_config\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_model_len\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    191\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache_config\u001B[38;5;241m.\u001B[39mnum_gpu_blocks \u001B[38;5;241m=\u001B[39m num_gpu_blocks\n\u001B[1;32m    192\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache_config\u001B[38;5;241m.\u001B[39mnum_cpu_blocks \u001B[38;5;241m=\u001B[39m num_cpu_blocks\n",
      "File \u001B[0;32m/projets/melodi/gsantoss/miniconda3/envs/myenv/lib/python3.11/site-packages/vllm/worker/worker.py:370\u001B[0m, in \u001B[0;36mraise_if_cache_size_invalid\u001B[0;34m(num_gpu_blocks, block_size, max_model_len)\u001B[0m\n\u001B[1;32m    367\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mraise_if_cache_size_invalid\u001B[39m(num_gpu_blocks, block_size,\n\u001B[1;32m    368\u001B[0m                                 max_model_len) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    369\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m num_gpu_blocks \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m--> 370\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo available memory for the cache blocks. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    371\u001B[0m                          \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTry increasing `gpu_memory_utilization` when \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    372\u001B[0m                          \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minitializing the engine.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    373\u001B[0m     max_seq_len \u001B[38;5;241m=\u001B[39m block_size \u001B[38;5;241m*\u001B[39m num_gpu_blocks\n\u001B[1;32m    374\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m max_model_len \u001B[38;5;241m>\u001B[39m max_seq_len:\n",
      "\u001B[0;31mValueError\u001B[0m: No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine."
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "base_ont = '/projets/melodi/gsantoss/data/oaei/tracks/complex/geolink'\n",
    "\n",
    "o1 = Graph().parse(f'{base_ont}/rdfgmo.rdf').serialize(format='ttl')\n",
    "o2 = Graph().parse(f'{base_ont}/rdfgbo.rdf').serialize(format='ttl')\n",
    "\n",
    "txt = f'''\n",
    "Given the two ontologies bellow:\n",
    "\n",
    "<ontology1>\n",
    "{o1}    \n",
    "</ontology1>    \n",
    "<ontology2>\n",
    "{o2}\n",
    "</ontology2>\n",
    "\n",
    "And one example of alignment between two different ontologies:\n",
    "\n",
    "<ontology1>\n",
    "@prefix lib: <http://example.org/library#> .\n",
    "@prefix dcterms: <http://purl.org/dc/terms/> .\n",
    "@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n",
    "\n",
    "lib:Book1 a lib:Book ;\n",
    "    dcterms:title \"The Catcher in the Rye\" ;\n",
    "    dcterms:creator lib:Author1 ;\n",
    "    lib:hasGenre \"Fiction\" .\n",
    "\n",
    "lib:Author1 a lib:Author ;\n",
    "    foaf:name \"J.D. Salinger\" ;\n",
    "    foaf:birthDate \"1919-01-01\" .\n",
    "</ontology1>\n",
    "<ontology2>\n",
    "@prefix pub: <http://example.org/publishing#> .\n",
    "@prefix dcterms: <http://purl.org/dc/terms/> .\n",
    "@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n",
    "\n",
    "pub:Book1 a pub:Book ;\n",
    "    dcterms:title \"To Kill a Mockingbird\" ;\n",
    "    dcterms:creator pub:Author1 ;\n",
    "    pub:publicationYear \"1960\" .\n",
    "\n",
    "pub:Author1 a pub:Author ;\n",
    "    foaf:name \"Harper Lee\" ;\n",
    "    pub:hasNationality \"American\" .\n",
    "</ontology2>\n",
    "<alignment>\n",
    "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
    "<rdf:RDF xmlns=\"http://knowledgeweb.semanticweb.org/heterogeneity/alignment\"\n",
    "         xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n",
    "         xmlns:xsd=\"http://www.w3.org/2001/XMLSchema#\"\n",
    "         xmlns:align=\"http://knowledgeweb.semanticweb.org/heterogeneity/alignment#\"\n",
    "         xmlns:edoal=\"http://ns.inria.org/edoal/1.0/#\">\n",
    "\n",
    "  <Alignment>\n",
    "    <xml>yes</xml>\n",
    "    <level>2EDOAL</level>\n",
    "    <type>**</type>\n",
    "    \n",
    "    <onto1>\n",
    "      <Ontology rdf:about=\"http://example.org/library#\"/>\n",
    "    </onto1>\n",
    "    <onto2>\n",
    "      <Ontology rdf:about=\"http://example.org/publishing#\"/>\n",
    "    </onto2>\n",
    "\n",
    "    <map>\n",
    "      <Cell>\n",
    "        <entity1 rdf:resource=\"http://example.org/library#Book\"/>\n",
    "        <entity2 rdf:resource=\"http://example.org/publishing#Book\"/>\n",
    "        <relation>=</relation>\n",
    "        <measure>1.0</measure>\n",
    "      </Cell>\n",
    "    </map>\n",
    "    <map>\n",
    "      <Cell>\n",
    "        <entity1 rdf:resource=\"http://example.org/library#Author\"/>\n",
    "        <entity2 rdf:resource=\"http://example.org/publishing#Author\"/>\n",
    "        <relation>=</relation>\n",
    "        <measure>1.0</measure>\n",
    "      </Cell>\n",
    "    </map>\n",
    "  </Alignment>\n",
    "</rdf:RDF>\n",
    "</alignment>\n",
    "\n",
    "Write a file in EDOAL format containing the complex alignment between the ontology1 and ontology2. You don't need to explain yourself. Just give as response the resulting file without saying anything. Here is one example bellow:\n",
    "'''"
   ],
   "id": "428947821bd42cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prompts = [\n",
    "    txt,\n",
    "]\n",
    "sampling_params = SamplingParams(temperature=0.0, top_p=1)"
   ],
   "id": "4e5799b4ccafb4f3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:56:45.794671Z",
     "start_time": "2024-07-02T15:56:45.765726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# Print the outputs.\n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    "    print(f\"Generated text: {generated_text!r}\")"
   ],
   "id": "22674977fc74196f",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'llm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mllm\u001B[49m\u001B[38;5;241m.\u001B[39mgenerate(prompts, sampling_params)\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Print the outputs.\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m output \u001B[38;5;129;01min\u001B[39;00m outputs:\n",
      "\u001B[0;31mNameError\u001B[0m: name 'llm' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T15:24:12.466648Z",
     "start_time": "2024-07-02T15:24:12.466319Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "392c98d910c53ae8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
