{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-05T09:43:04.505243Z",
     "start_time": "2024-07-05T09:42:59.882003Z"
    }
   },
   "source": [
    "import gc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from safetensors.torch import load_file\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import math\n",
    "from accelerate import init_empty_weights\n",
    "from rdflib import Graph"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T09:43:05.251974Z",
     "start_time": "2024-07-05T09:43:04.506750Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "shards = ['model-00001-of-00004.safetensors', 'model-00002-of-00004.safetensors', 'model-00003-of-00004.safetensors',\n",
    "          'model-00004-of-00004.safetensors']\n",
    "\n",
    "base_path = \"/users/melodi/gsantoss/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/\"\n",
    "\n",
    "state_dict = {}\n",
    "for shard in shards:\n",
    "    state_dict.update(load_file(base_path + shard))\n",
    "\n",
    "with open(base_path + \"config.json\") as f:\n",
    "    config = json.load(f)"
   ],
   "id": "6f5f4c7aea46c78e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T09:43:05.255726Z",
     "start_time": "2024-07-05T09:43:05.253078Z"
    }
   },
   "cell_type": "code",
   "source": "config['mlp_bias'] = False",
   "id": "8236189972cce71b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T09:43:05.273553Z",
     "start_time": "2024-07-05T09:43:05.257587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for k, v in config.items():\n",
    "    print(k, v)"
   ],
   "id": "cb7d6753d672fb0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architectures ['LlamaForCausalLM']\n",
      "attention_bias False\n",
      "attention_dropout 0.0\n",
      "bos_token_id 128000\n",
      "eos_token_id 128009\n",
      "hidden_act silu\n",
      "hidden_size 4096\n",
      "initializer_range 0.02\n",
      "intermediate_size 14336\n",
      "max_position_embeddings 8192\n",
      "model_type llama\n",
      "num_attention_heads 32\n",
      "num_hidden_layers 32\n",
      "num_key_value_heads 8\n",
      "pretraining_tp 1\n",
      "rms_norm_eps 1e-05\n",
      "rope_scaling None\n",
      "rope_theta 500000.0\n",
      "tie_word_embeddings False\n",
      "torch_dtype bfloat16\n",
      "transformers_version 4.40.0.dev0\n",
      "use_cache True\n",
      "vocab_size 128256\n",
      "mlp_bias False\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T09:43:05.512942Z",
     "start_time": "2024-07-05T09:43:05.274755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_ont = '/projets/melodi/gsantoss/data/oaei/tracks/complex/geolink'\n",
    "\n",
    "o1 = Graph().parse(f'{base_ont}/rdfgmo.rdf').serialize(format='ttl')\n",
    "o2 = Graph().parse(f'{base_ont}/rdfgbo.rdf').serialize(format='ttl')\n",
    "\n",
    "txt = f'''\n",
    "Given the two ontologies bellow:\n",
    "\n",
    "<ontology1>\n",
    "{o1}    \n",
    "</ontology1>    \n",
    "<ontology2>\n",
    "{o2}\n",
    "</ontology2>\n",
    "\n",
    "And one example of alignment between two different ontologies:\n",
    "\n",
    "<ontology1>\n",
    "@prefix lib: <http://example.org/library#> .\n",
    "@prefix dcterms: <http://purl.org/dc/terms/> .\n",
    "@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n",
    "\n",
    "lib:Book1 a lib:Book ;\n",
    "    dcterms:title \"The Catcher in the Rye\" ;\n",
    "    dcterms:creator lib:Author1 ;\n",
    "    lib:hasGenre \"Fiction\" .\n",
    "\n",
    "lib:Author1 a lib:Author ;\n",
    "    foaf:name \"J.D. Salinger\" ;\n",
    "    foaf:birthDate \"1919-01-01\" .\n",
    "</ontology1>\n",
    "<ontology2>\n",
    "@prefix pub: <http://example.org/publishing#> .\n",
    "@prefix dcterms: <http://purl.org/dc/terms/> .\n",
    "@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n",
    "\n",
    "pub:Book1 a pub:Book ;\n",
    "    dcterms:title \"To Kill a Mockingbird\" ;\n",
    "    dcterms:creator pub:Author1 ;\n",
    "    pub:publicationYear \"1960\" .\n",
    "\n",
    "pub:Author1 a pub:Author ;\n",
    "    foaf:name \"Harper Lee\" ;\n",
    "    pub:hasNationality \"American\" .\n",
    "</ontology2>\n",
    "<alignment>\n",
    "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
    "<rdf:RDF xmlns=\"http://knowledgeweb.semanticweb.org/heterogeneity/alignment\"\n",
    "         xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n",
    "         xmlns:xsd=\"http://www.w3.org/2001/XMLSchema#\"\n",
    "         xmlns:align=\"http://knowledgeweb.semanticweb.org/heterogeneity/alignment#\"\n",
    "         xmlns:edoal=\"http://ns.inria.org/edoal/1.0/#\">\n",
    "\n",
    "  <Alignment>\n",
    "    <xml>yes</xml>\n",
    "    <level>2EDOAL</level>\n",
    "    <type>**</type>\n",
    "    \n",
    "    <onto1>\n",
    "      <Ontology rdf:about=\"http://example.org/library#\"/>\n",
    "    </onto1>\n",
    "    <onto2>\n",
    "      <Ontology rdf:about=\"http://example.org/publishing#\"/>\n",
    "    </onto2>\n",
    "\n",
    "    <map>\n",
    "      <Cell>\n",
    "        <entity1 rdf:resource=\"http://example.org/library#Book\"/>\n",
    "        <entity2 rdf:resource=\"http://example.org/publishing#Book\"/>\n",
    "        <relation>=</relation>\n",
    "        <measure>1.0</measure>\n",
    "      </Cell>\n",
    "    </map>\n",
    "    <map>\n",
    "      <Cell>\n",
    "        <entity1 rdf:resource=\"http://example.org/library#Author\"/>\n",
    "        <entity2 rdf:resource=\"http://example.org/publishing#Author\"/>\n",
    "        <relation>=</relation>\n",
    "        <measure>1.0</measure>\n",
    "      </Cell>\n",
    "    </map>\n",
    "  </Alignment>\n",
    "</rdf:RDF>\n",
    "</alignment>\n",
    "\n",
    "Write a file in EDOAL format containing the complex alignment between the ontology1 and ontology2. You don't need to explain yourself. Just give as response the resulting file without saying anything. Here is one example bellow:\n",
    "'''"
   ],
   "id": "9c802ec11be5bd2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T09:43:05.559018Z",
     "start_time": "2024-07-05T09:43:05.514101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_prompt = '''apple: fruit\n",
    "orange: fruit\n",
    "zucchini: vegetable\n",
    "tomato:\n",
    "\n",
    "Complete this list'''\n",
    "\n",
    "messages = [\n",
    "    # {\"role\": \"system\", \"content\": \"You are an Ontology Alignment expert. You are able to align two ontologies by creating a file in EDOAL format containing the result alignments. You are able to produce complex alignments that are those involving multiple entities and relationships in a n:m cardinality. The user will provide you with two ontologies and you respond with the EDOAL file containing the alignments. You don't need to explain yourself. Just give as response the resulting file without saying anything.\"},\n",
    "    {\"role\": \"user\", \"content\": sample_prompt},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(input_ids.shape)"
   ],
   "id": "e05a856a4042c1b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 29])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T09:49:35.244990Z",
     "start_time": "2024-07-05T09:49:35.010459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n",
    "        super().__init__()\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        self.max_seq_len_cached = max_position_embeddings\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "\n",
    "        device_type = x.device.type\n",
    "        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n",
    "        with torch.autocast(device_type=device_type, enabled=False):\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            cos = emb.cos()\n",
    "            sin = emb.sin()\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    \n",
    "    x1 = x.copy()\n",
    "    x1.output_file = 'x1'\n",
    "    x2 = x.copy()\n",
    "    x2.output_file = 'x2'\n",
    "    \n",
    "    x1 = x1[..., : x1.shape[-1] // 2]\n",
    "    x2 = x2[..., x2.shape[-1] // 2:]\n",
    "            \n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, unsqueeze_dim=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    \n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "\n",
    "class SdpaAttention(nn.Module):\n",
    "    def __init__(self, layer_idx, config, torch_dtype=torch.float32):\n",
    "        super(SdpaAttention, self).__init__()\n",
    "        self.layer_idx = layer_idx\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        self.num_heads = config['num_attention_heads']\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        self.num_key_value_heads = config['num_key_value_heads']\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config['attention_bias'],\n",
    "                                dtype=torch_dtype)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim,\n",
    "                                bias=config['attention_bias'], dtype=torch_dtype)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim,\n",
    "                                bias=config['attention_bias'], dtype=torch_dtype)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config['attention_bias'], dtype=torch_dtype)\n",
    "        self.max_position_embeddings = config['max_position_embeddings']\n",
    "        self.rope_theta = config['rope_theta']\n",
    "        self.rotary_emb = RotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings,\n",
    "                                          base=self.rope_theta)\n",
    "        self.attention_dropout = config['attention_dropout']\n",
    "\n",
    "    def forward(self, x, position_ids, kv_cache=None):\n",
    "        bsz, q_len, _ = x.size()\n",
    "        \n",
    "        query_states = x.copy()\n",
    "        query_states.output_file = 'query'\n",
    "        \n",
    "        key_states = x.copy()\n",
    "        key_states.output_file = 'key'\n",
    "        \n",
    "        value_states = x.copy()\n",
    "        value_states.output_file = 'value'\n",
    "        \n",
    "        query_states = self.q_proj(query_states)\n",
    "        key_states = self.k_proj(key_states)\n",
    "        value_states = self.v_proj(value_states)\n",
    "        \n",
    "        query_states = query_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, -1, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, -1, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        cos, sin = self.rotary_emb(value_states, position_ids)\n",
    "    \n",
    "        \n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "        \n",
    "    \n",
    "        if kv_cache is not None:\n",
    "            if kv_cache[self.layer_idx] is not None:\n",
    "                past_key, past_value = kv_cache[self.layer_idx]\n",
    "                key_states = torch.cat([past_key, key_states], dim=2)\n",
    "                value_states = torch.cat([past_value, value_states], dim=2)\n",
    "            kv_cache[self.layer_idx] = (key_states, value_states)\n",
    "\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "\n",
    "\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        causal_mask = torch.triu(torch.full((q_len, q_len), -1e9), diagonal=1)\n",
    "        attn_weights = attn_weights + causal_mask\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.reshape(bsz, q_len, -1)\n",
    "\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "class LLamaMLP(nn.Module):\n",
    "    def __init__(self, config, torch_dtype=torch.float32):\n",
    "        super(LLamaMLP, self).__init__()\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        self.intermediate_size = config['intermediate_size']\n",
    "\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config['mlp_bias'], dtype=torch_dtype)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config['mlp_bias'], dtype=torch_dtype)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config['mlp_bias'], dtype=torch_dtype)\n",
    "\n",
    "        self.act_fn = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, torch_dtype, eps=1e-6):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size, dtype=torch_dtype))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_dtype = x.dtype\n",
    "        hidden_states = x.to(torch.float32)\n",
    "        variance = hidden_states.copy()\n",
    "        variance.output_file = 'var'\n",
    "        variance = variance.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return hidden_states.to(input_dtype) * self.weight\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, layer_idx, config, torch_dtype=torch.float32):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.layer_idx = layer_idx\n",
    "        self.input_layernorm = RMSNorm(config['hidden_size'], torch_dtype, eps=config['rms_norm_eps'])\n",
    "\n",
    "        self.self_attn = SdpaAttention(layer_idx, config, torch_dtype=torch_dtype)\n",
    "\n",
    "        self.post_attention_layernorm = RMSNorm(config['hidden_size'], torch_dtype, eps=config['rms_norm_eps'])\n",
    "        self.mlp = LLamaMLP(config, torch_dtype)\n",
    "\n",
    "    def forward(self, x, position_ids, kv_cache=None):\n",
    "        residual = x.copy()\n",
    "        \n",
    "        hidden_states = self.input_layernorm(x)\n",
    "        \n",
    "    \n",
    "        attention_output = self.self_attn(hidden_states, position_ids, kv_cache=kv_cache)\n",
    "        hidden_states = residual + attention_output\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = residual + self.mlp(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class LLama(nn.Module):\n",
    "    def __init__(self, config, torch_dtype=torch.float32):\n",
    "        super(LLama, self).__init__()\n",
    "        self.padding_idx = config['eos_token_id']\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config['vocab_size'], config['hidden_size'], padding_idx=self.padding_idx,\n",
    "                                         dtype=torch_dtype)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderLayer(layer_idx, config, torch_dtype) for layer_idx in range(config['num_hidden_layers'])])\n",
    "\n",
    "        self.norm = RMSNorm(config['hidden_size'], torch_dtype, eps=config['rms_norm_eps'])\n",
    "\n",
    "    def forward(self, x, position_ids=None, kv_cache=None):\n",
    "        \n",
    "        \n",
    "        hidden_states = self.embed_tokens(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states, position_ids, kv_cache=kv_cache)\n",
    "            gc.collect()\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class LLamaGenerator(nn.Module):\n",
    "    def __init__(self, config, torch_dtype=torch.float32):\n",
    "        super(LLamaGenerator, self).__init__()\n",
    "        self.config = config\n",
    "        self.model = LLama(config, torch_dtype=torch_dtype)\n",
    "        self.lm_head = nn.Linear(config['hidden_size'], config['vocab_size'], dtype=torch_dtype, bias=False)\n",
    "\n",
    "    def forward(self, x, max_length=10, stop_token=None):\n",
    "        inp = x\n",
    "        fo = []\n",
    "        kv_cache = [None] * self.config['num_hidden_layers']\n",
    "        position_ids = torch.arange(inp.size(1)).unsqueeze(0).expand(x.size(0), -1)\n",
    "        with torch.no_grad():\n",
    "            for _ in tqdm(range(max_length)):\n",
    "                out = self.model(inp, position_ids=position_ids, kv_cache=kv_cache)\n",
    "                out = self.lm_head(out)[:, -1, :]\n",
    "                out = out.argmax(-1).unsqueeze(-1)\n",
    "                fo.append(out.item())\n",
    "                if stop_token is not None and out.item() == stop_token:\n",
    "                    break\n",
    "                inp = out\n",
    "                position_ids = torch.unsqueeze(position_ids[:, -1] + 1, 0)\n",
    "\n",
    "        return fo\n",
    "\n",
    "\n",
    "with init_empty_weights():\n",
    "    generator = LLamaGenerator(config, torch_dtype=torch.bfloat16)\n",
    "\n",
    "generator.load_state_dict(state_dict, assign=True)\n",
    "generator.eval()\n",
    "\n"
   ],
   "id": "ca8d157fdddfaea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLamaGenerator(\n",
       "  (model): LLama(\n",
       "    (embed_tokens): Embedding(128256, 4096, padding_idx=128009)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x DecoderLayer(\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (self_attn): SdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (mlp): LLamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T09:49:38.286905Z",
     "start_time": "2024-07-05T09:49:36.885217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MemoryMappedTensor(object):\n",
    "    def __init__(self, data, save=False, skip=False, seq_dim=1, base_path='/projets/melodi/gsantoss/tmp/tmpe',\n",
    "                 input_file='ie',\n",
    "                 output_file='ie', msize=500_000, **kwargs):\n",
    "        self.t = data.clone().to('meta')\n",
    "        self.base_path = base_path\n",
    "        self.input_file = input_file\n",
    "        self.output_file = output_file\n",
    "        self.msize = msize\n",
    "        self.seq_dim = seq_dim\n",
    "        self.skip = skip\n",
    "        if save and not skip:\n",
    "            for i, sl in enumerate(torch.split(self.t, msize, dim=seq_dim)):\n",
    "                torch.save(sl, f'{base_path}/{input_file}_{i}.pt')\n",
    "        \n",
    "        self.current_dtype = self.t.dtype\n",
    "        self.save = save\n",
    "\n",
    "    def load_slices(self):\n",
    "        for i, sl in enumerate(torch.split(self.t, self.msize, dim=self.seq_dim)):\n",
    "            yield i, torch.load(f'{self.base_path}/{self.input_file}_{i}.pt')\n",
    "            \n",
    "    def slice_count(self):\n",
    "        return len(list(torch.split(self.t, self.msize, dim=self.seq_dim)))\n",
    "            \n",
    "    def tensor_map(self, func):\n",
    "        \n",
    "        m = self.copy()\n",
    "        res = func(self.t)\n",
    "        if not self.skip:\n",
    "            for i, sl in self.load_slices():\n",
    "                out = func(sl)\n",
    "                torch.save(out, f'{self.base_path}/{self.output_file}_{i}.pt')\n",
    "                \n",
    "        m.input_file = m.output_file\n",
    "        m.t = res\n",
    "        return m\n",
    "        \n",
    "    def stream_tensor_map(self, func, other, stream):\n",
    "        m = self.copy()\n",
    "        res = func(self.t.to('meta'), other.to('meta'))\n",
    "        if not self.skip:\n",
    "            for (i, sl), (i2, sl2) in zip(self.load_slices(), stream):\n",
    "                \n",
    "                out = func(sl, sl2)\n",
    "                torch.save(out, f'{self.base_path}/{self.output_file}_{i}.pt')\n",
    "                \n",
    "        m.input_file = m.output_file\n",
    "        m.t = res\n",
    "        return m\n",
    "        \n",
    "        \n",
    "\n",
    "    @classmethod\n",
    "    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
    "        if kwargs is None:\n",
    "            kwargs = {}\n",
    "        if func.__name__ in {'embedding', 'linear'}:\n",
    "            mt = args[0]\n",
    "            return mt.tensor_map(lambda x: func(x, *args[1:], **kwargs))\n",
    "\n",
    "        elif func.__name__ == 'rsqrt':\n",
    "            return args[0].tensor_map(lambda x: func(x, **kwargs))\n",
    "        \n",
    "        elif func.__name__ == 'cat':\n",
    "            if 'dim' in kwargs:\n",
    "                dim = kwargs['dim']\n",
    "                if len(args[0]) > 2:\n",
    "                    raise Exception('more than 2 tensors in cat')\n",
    "                if dim != args[0][0].seq_dim:\n",
    "                    return args[0][0].stream_tensor_map(lambda x, y: func([x, y], **kwargs), args[0][1].t, args[0][1].load_slices())\n",
    "                else:\n",
    "                    raise Exception('cat in seq dim')\n",
    "            raise Exception('no dim')\n",
    "\n",
    "        \n",
    "        elif func.__name__ == 'add':\n",
    "            raise Exception('not implemented')\n",
    "\n",
    "            if kwargs is None:\n",
    "                kwargs = {}\n",
    "            res = func(args[0].t, args[1].t, **kwargs).to('meta')\n",
    "            if not args[0].skip:\n",
    "                for i, (sl1, sl2) in zip(args[0].load_slices(), args[1].load_slices()):\n",
    "                    out = func(sl1, sl2, **kwargs)\n",
    "                    torch.save(out, f'{args[0].base_path}/{args[0].output_file}_{i}.pt')\n",
    "\n",
    "            m = args[0].copy()\n",
    "            m.input_file = m.output_file\n",
    "            m.t = res\n",
    "            return m\n",
    "        \n",
    "        elif func.__name__ == 'matmul':\n",
    "\n",
    "            if kwargs is None:\n",
    "                kwargs = {}\n",
    "            res = func(args[0].t, args[1].t, **kwargs).to('meta')\n",
    "            if not args[0].skip:\n",
    "                print('huehue')\n",
    "                print(args[0].t.shape, args[1].t.shape)\n",
    "                for (i, sl1), (i2, sl2) in zip(args[0].load_slices(), args[1].load_slices()):\n",
    "                    print(i, sl1.shape, i2, sl2.shape)\n",
    "                    raise Exception('hue')\n",
    "                    out = func(sl1, sl2, **kwargs)\n",
    "                    torch.save(out, f'{args[0].base_path}/{args[0].output_file}_{i}.pt')\n",
    "\n",
    "            m = args[0].copy()\n",
    "            m.input_file = m.output_file\n",
    "            m.t = res\n",
    "            return m\n",
    "        \n",
    "        elif func.__name__ == 'softmax':\n",
    "\n",
    "            return args[0].tensor_map(lambda x: func(x, **kwargs))\n",
    "        \n",
    "        elif func.__name__ == 'dropout':\n",
    "\n",
    "            return args[0].tensor_map(lambda x: func(x, **kwargs))\n",
    "        \n",
    "        elif func.__name__ == 'silu':\n",
    "            return args[0].tensor_map(lambda x: func(x, **kwargs))\n",
    "\n",
    "        print(func)\n",
    "        print(func.__name__, types)\n",
    "        raise Exception('hue')\n",
    "        return None\n",
    "\n",
    "    def size(self, dim=None):\n",
    "        return self.t.size(dim)\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.current_dtype\n",
    "\n",
    "    def to(self, device):\n",
    "\n",
    "        if type(device) == torch.dtype:\n",
    "            if self.current_dtype == device:\n",
    "                return self\n",
    "            else:\n",
    "                m = self.tensor_map(lambda x: x.to(device))\n",
    "                m.current_dtype = device\n",
    "                return m\n",
    "\n",
    "        elif type(device) == torch.device:\n",
    "            raise Exception('hue')\n",
    "\n",
    "        return None\n",
    "\n",
    "    def pow(self, exp):\n",
    "        return self.tensor_map(lambda x: x.pow(exp))\n",
    "\n",
    "    def copy(self, save=False):\n",
    "        m = MemoryMappedTensor(self.t, save=save, skip=self.skip, seq_dim=self.seq_dim,\n",
    "                               input_file=self.input_file, output_file=self.output_file, msize=self.msize)\n",
    "        m.current_dtype = self.current_dtype\n",
    "        return m\n",
    "\n",
    "    def mean(self, dim, keepdim=False):\n",
    "        \n",
    "        if dim != self.seq_dim:\n",
    "            return self.tensor_map(lambda x: x.mean(dim, keepdim=keepdim))\n",
    "        \n",
    "        else:         \n",
    "            shape = list(self.t.shape)\n",
    "            sl = shape[dim]\n",
    "            shape[dim] = 1\n",
    "            fr = torch.zeros(shape, dtype=self.t.dtype)       \n",
    "\n",
    "            if not self.skip:\n",
    "                for i, sl in self.load_slices():\n",
    "                    fr += sl.sum(dim, keepdim=True)\n",
    "\n",
    "            fr /= sl\n",
    "            \n",
    "            torch.save(fr, f'{self.base_path}/{self.output_file}_0.pt')\n",
    "            \n",
    "            m = self.copy()\n",
    "            m.input_file = m.output_file\n",
    "            m.t = self.t.mean(dim, keepdim=keepdim)\n",
    "            return m\n",
    "\n",
    "    def __add__(self, other):\n",
    "\n",
    "        if type(other) == float:\n",
    "            other = torch.as_tensor(other)\n",
    "            \n",
    "            return self.tensor_map(lambda x: x + other)\n",
    "            \n",
    "        elif type(other) == MemoryMappedTensor:\n",
    "            if other.t.shape[other.seq_dim] > 1:\n",
    "                \n",
    "                return self.stream_tensor_map(lambda x, y: x + y, other.t, other.load_slices())        \n",
    "        \n",
    "            else:                \n",
    "                sl = list(other.load_slices())[0][1]                \n",
    "                return self.tensor_map(lambda x: x + sl)\n",
    "            \n",
    "        else:\n",
    "            return self.tensor_map(lambda x: x + other if x.device != torch.device('meta') else x + other.to('meta'))\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        if type(other) == float:\n",
    "            other = torch.as_tensor(other)\n",
    "            \n",
    "            return self.tensor_map(lambda x: x * other)\n",
    "            \n",
    "        elif type(other) == MemoryMappedTensor:\n",
    "            if other.t.shape[other.seq_dim] > 1:\n",
    "                \n",
    "                return self.stream_tensor_map(lambda x, y: x * y, other.t, other.load_slices())        \n",
    "        \n",
    "            else:                \n",
    "                sl = list(other.load_slices())[0][1]                \n",
    "                return self.tensor_map(lambda x: x * sl)\n",
    "            \n",
    "        else:\n",
    "            return self.tensor_map(lambda x: x * other if x.device != torch.device('meta') else x * other.to('meta'))\n",
    "\n",
    "    def view(self, *args):\n",
    "        return self.tensor_map(lambda x: x.view(*args))\n",
    "\n",
    "    def transpose(self, *args):\n",
    "        \n",
    "        res = self.t.transpose(*args).to('meta')\n",
    "        if not self.skip:        \n",
    "            for i, sl in self.load_slices():\n",
    "                out = sl.transpose(*args)\n",
    "                \n",
    "                for j, sl2 in enumerate(torch.split(out, self.msize, dim=1)):\n",
    "                    if i == 0:\n",
    "                        torch.save(sl2, f'{self.base_path}/tcache_{j}.pt')\n",
    "                    else:\n",
    "                        tl = torch.load(f'{self.base_path}/tcache_{j}.pt')\n",
    "                        tl = torch.cat([tl, sl2], dim=2)\n",
    "                        torch.save(tl, f'{self.base_path}/tcache_{j}.pt')\n",
    "                        \n",
    "            \n",
    "            for i, sl in enumerate(torch.split(res, self.msize, dim=1)):\n",
    "                tl = torch.load(f'{self.base_path}/tcache_{i}.pt')\n",
    "                torch.save(tl, f'{self.base_path}/{self.output_file}_{i}.pt')\n",
    "        \n",
    "        \n",
    "        m = self.copy()\n",
    "        m.input_file = m.output_file\n",
    "        m.t = res\n",
    "        m.current_dtype = res.dtype\n",
    "        \n",
    "        return m\n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "    @property\n",
    "    def shape(self):\n",
    "        return self.t.shape\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensor_map(lambda x: x[idx])\n",
    "    \n",
    "    def __neg__(self):\n",
    "        return self.tensor_map(lambda x: -x)    \n",
    "    \n",
    "    def expand(self, *args):\n",
    "        return self.tensor_map(lambda x: x.expand(*args))\n",
    "    \n",
    "    def reshape(self, *args):\n",
    "        return self.tensor_map(lambda x: x.reshape(*args))\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        if type(other) == float:\n",
    "            other = torch.as_tensor(other)\n",
    "            \n",
    "            return self.tensor_map(lambda x: x / other)\n",
    "            \n",
    "        elif type(other) == MemoryMappedTensor:\n",
    "            if other.t.shape[other.seq_dim] > 1:\n",
    "                \n",
    "                return self.stream_tensor_map(lambda x, y: x / y, other.t, other.load_slices())        \n",
    "        \n",
    "            else:                \n",
    "                sl = list(other.load_slices())[0][1]                \n",
    "                return self.tensor_map(lambda x: x / sl)\n",
    "            \n",
    "        else:\n",
    "            return self.tensor_map(lambda x: x / other if x.device != torch.device('meta') else x / other.to('meta'))\n",
    "    \n",
    "    \n",
    "    def contiguous(self):\n",
    "        self.t.contiguous()\n",
    "        return self\n",
    "    \n",
    "    def argmax(self, dim):\n",
    "        return self.tensor_map(lambda x: x.argmax(dim))\n",
    "    \n",
    "    def unsqueeze(self, dim):\n",
    "        return self.tensor_map(lambda x: x.unsqueeze(dim))\n",
    "    \n",
    "    def item(self):\n",
    "        for i, sl in self.load_slices():\n",
    "            return sl.item()\n",
    "\n",
    "mm_tensor = MemoryMappedTensor(input_ids, save=True, msize=10)\n",
    "\n",
    "# out = generator(input_ids)\n",
    "out = generator(mm_tensor)\n",
    "\n",
    "print(out)\n",
    "tokenizer.decode(out)"
   ],
   "id": "2a55bd3ad9415728",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "198b888d1e0c4d54b4d1903095f4ce62"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key states torch.Size([1, 32, 29, 128])\n",
      "torch.Size([1, 32, 29, 128])\n",
      "torch.Size([1, 10, 8, 128])\n",
      "torch.Size([1, 9, 8, 128])\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/projets/melodi/gsantoss/tmp/tmpe/key_3.pt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 307\u001B[0m\n\u001B[1;32m    304\u001B[0m mm_tensor \u001B[38;5;241m=\u001B[39m MemoryMappedTensor(input_ids, save\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, msize\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m    306\u001B[0m \u001B[38;5;66;03m# out = generator(input_ids)\u001B[39;00m\n\u001B[0;32m--> 307\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[43mgenerator\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmm_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    309\u001B[0m \u001B[38;5;28mprint\u001B[39m(out)\n\u001B[1;32m    310\u001B[0m tokenizer\u001B[38;5;241m.\u001B[39mdecode(out)\n",
      "File \u001B[0;32m/projets/melodi/gsantoss/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/projets/melodi/gsantoss/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[16], line 243\u001B[0m, in \u001B[0;36mLLamaGenerator.forward\u001B[0;34m(self, x, max_length, stop_token)\u001B[0m\n\u001B[1;32m    241\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m    242\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(max_length)):\n\u001B[0;32m--> 243\u001B[0m         out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkv_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkv_cache\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    244\u001B[0m         out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlm_head(out)[:, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :]\n\u001B[1;32m    245\u001B[0m         out \u001B[38;5;241m=\u001B[39m out\u001B[38;5;241m.\u001B[39margmax(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m/projets/melodi/gsantoss/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/projets/melodi/gsantoss/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[16], line 221\u001B[0m, in \u001B[0;36mLLama.forward\u001B[0;34m(self, x, position_ids, kv_cache)\u001B[0m\n\u001B[1;32m    218\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membed_tokens(x)\n\u001B[1;32m    220\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[0;32m--> 221\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkv_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkv_cache\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    222\u001B[0m     gc\u001B[38;5;241m.\u001B[39mcollect()\n\u001B[1;32m    224\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(hidden_states)\n",
      "File \u001B[0;32m/projets/melodi/gsantoss/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/projets/melodi/gsantoss/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[16], line 192\u001B[0m, in \u001B[0;36mDecoderLayer.forward\u001B[0;34m(self, x, position_ids, kv_cache)\u001B[0m\n\u001B[1;32m    187\u001B[0m residual \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m    189\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_layernorm(x)\n\u001B[0;32m--> 192\u001B[0m attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mself_attn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkv_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkv_cache\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    193\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m residual \u001B[38;5;241m+\u001B[39m attention_output\n\u001B[1;32m    195\u001B[0m residual \u001B[38;5;241m=\u001B[39m hidden_states\n",
      "File \u001B[0;32m/projets/melodi/gsantoss/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/projets/melodi/gsantoss/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[16], line 119\u001B[0m, in \u001B[0;36mSdpaAttention.forward\u001B[0;34m(self, x, position_ids, kv_cache)\u001B[0m\n\u001B[1;32m    115\u001B[0m value_states \u001B[38;5;241m=\u001B[39m repeat_kv(value_states, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_key_value_groups)\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkey states\u001B[39m\u001B[38;5;124m'\u001B[39m, key_states\u001B[38;5;241m.\u001B[39mt\u001B[38;5;241m.\u001B[39mshape)\n\u001B[0;32m--> 119\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mkey_states\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_slices\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m    120\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mprint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalue states\u001B[39m\u001B[38;5;124m'\u001B[39m, value_states\u001B[38;5;241m.\u001B[39mt\u001B[38;5;241m.\u001B[39mshape)\n",
      "Cell \u001B[0;32mIn[17], line 21\u001B[0m, in \u001B[0;36mMemoryMappedTensor.load_slices\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_slices\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, sl \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(torch\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mt, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmsize, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseq_dim)):\n\u001B[0;32m---> 21\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m i, \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbase_path\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minput_file\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mi\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m.pt\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/projets/melodi/gsantoss/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/serialization.py:997\u001B[0m, in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[1;32m    994\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m    995\u001B[0m     pickle_load_args[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m--> 997\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[1;32m    998\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[1;32m    999\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[1;32m   1000\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[1;32m   1001\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[1;32m   1002\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[0;32m/projets/melodi/gsantoss/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/serialization.py:444\u001B[0m, in \u001B[0;36m_open_file_like\u001B[0;34m(name_or_buffer, mode)\u001B[0m\n\u001B[1;32m    442\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[1;32m    443\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[0;32m--> 444\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    445\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    446\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[0;32m/projets/melodi/gsantoss/miniconda3/envs/myenv/lib/python3.11/site-packages/torch/serialization.py:425\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[0;34m(self, name, mode)\u001B[0m\n\u001B[1;32m    424\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[0;32m--> 425\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mopen\u001B[39m(name, mode))\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/projets/melodi/gsantoss/tmp/tmpe/key_3.pt'"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T09:43:07.289552Z",
     "start_time": "2024-07-05T09:43:07.289413Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "971b6ada07340e5f",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
