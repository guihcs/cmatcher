{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-11T13:54:44.375860Z",
     "start_time": "2024-06-11T13:54:07.876826Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from safetensors.torch import load_file\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import math\n",
    "from accelerate import init_empty_weights"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:54:45.488407Z",
     "start_time": "2024-06-11T13:54:44.563337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "shards = ['model-00001-of-00004.safetensors', 'model-00002-of-00004.safetensors', 'model-00003-of-00004.safetensors',\n",
    "          'model-00004-of-00004.safetensors']\n",
    "\n",
    "base_path = \"/users/melodi/gsantoss/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/\"\n",
    "\n",
    "state_dict = {}\n",
    "for shard in shards:\n",
    "    state_dict.update(load_file(base_path + shard))\n",
    "\n",
    "with open(base_path + \"config.json\") as f:\n",
    "    config = json.load(f)"
   ],
   "id": "6f5f4c7aea46c78e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:54:45.492488Z",
     "start_time": "2024-06-11T13:54:45.489602Z"
    }
   },
   "cell_type": "code",
   "source": "config['mlp_bias'] = False",
   "id": "8236189972cce71b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:54:45.509532Z",
     "start_time": "2024-06-11T13:54:45.494278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for k, v in config.items():\n",
    "    print(k, v)"
   ],
   "id": "cb7d6753d672fb0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architectures ['LlamaForCausalLM']\n",
      "attention_bias False\n",
      "attention_dropout 0.0\n",
      "bos_token_id 128000\n",
      "eos_token_id 128009\n",
      "hidden_act silu\n",
      "hidden_size 4096\n",
      "initializer_range 0.02\n",
      "intermediate_size 14336\n",
      "max_position_embeddings 8192\n",
      "model_type llama\n",
      "num_attention_heads 32\n",
      "num_hidden_layers 32\n",
      "num_key_value_heads 8\n",
      "pretraining_tp 1\n",
      "rms_norm_eps 1e-05\n",
      "rope_scaling None\n",
      "rope_theta 500000.0\n",
      "tie_word_embeddings False\n",
      "torch_dtype bfloat16\n",
      "transformers_version 4.40.0.dev0\n",
      "use_cache True\n",
      "vocab_size 128256\n",
      "mlp_bias False\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:54:45.914548Z",
     "start_time": "2024-06-11T13:54:45.510674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_prompt = '''apple: fruit\n",
    "orange: fruit\n",
    "zucchini: vegetable\n",
    "tomato:\n",
    "\n",
    "Complete this list'''\n",
    "\n",
    "messages = [\n",
    "    # {\"role\": \"system\", \"content\": \"You are an Ontology Alignment expert. You are able to align two ontologies by creating a file in EDOAL format containing the result alignments. You are able to produce complex alignments that are those involving multiple entities and relationships in a n:m cardinality. The user will provide you with two ontologies and you respond with the EDOAL file containing the alignments. You don't need to explain yourself. Just give as response the resulting file without saying anything.\"},\n",
    "    {\"role\": \"user\", \"content\": sample_prompt},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(input_ids.shape)"
   ],
   "id": "e05a856a4042c1b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 29])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:54:46.016095Z",
     "start_time": "2024-06-11T13:54:45.915801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Cache:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n",
    "        super().__init__()\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        self.max_seq_len_cached = max_position_embeddings\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "\n",
    "        device_type = x.device.type\n",
    "        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n",
    "        with torch.autocast(device_type=device_type, enabled=False):\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            cos = emb.cos()\n",
    "            sin = emb.sin()\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "\n",
    "class SdpaAttention(nn.Module):\n",
    "    def __init__(self, layer_idx, config, torch_dtype=torch.float32):\n",
    "        super(SdpaAttention, self).__init__()\n",
    "        self.layer_idx = layer_idx\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        self.num_heads = config['num_attention_heads']\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        self.num_key_value_heads = config['num_key_value_heads']\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "\n",
    "    \n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config['attention_bias'],\n",
    "                                dtype=torch_dtype)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim,\n",
    "                                bias=config['attention_bias'], dtype=torch_dtype)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim,\n",
    "                                bias=config['attention_bias'], dtype=torch_dtype)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config['attention_bias'], dtype=torch_dtype)\n",
    "        self.max_position_embeddings = config['max_position_embeddings']\n",
    "        self.rope_theta = config['rope_theta']\n",
    "        self.rotary_emb = RotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings, base=self.rope_theta)\n",
    "        self.attention_dropout = config['attention_dropout']\n",
    "\n",
    "    def forward(self, x, position_ids, kv_cache=None):\n",
    "        bsz, q_len, _ = x.size()\n",
    "\n",
    "        query_states = self.q_proj(x)\n",
    "        key_states = self.k_proj(x)\n",
    "        value_states = self.v_proj(x)\n",
    "        \n",
    "        query_states = query_states.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, -1, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, -1, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        cos, sin = self.rotary_emb(value_states, position_ids)\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "        \n",
    "        if kv_cache is not None:\n",
    "            if kv_cache[self.layer_idx] is not None:\n",
    "                past_key, past_value = kv_cache[self.layer_idx]\n",
    "                key_states = torch.cat([past_key, key_states], dim=2)\n",
    "                value_states = torch.cat([past_value, value_states], dim=2)\n",
    "            kv_cache[self.layer_idx] = (key_states, value_states)\n",
    "        \n",
    "\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "        \n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        causal_mask = torch.triu(torch.full((q_len, q_len), -1e9), diagonal=1)\n",
    "        attn_weights = attn_weights + causal_mask\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(bsz, q_len, -1)\n",
    "\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "class LLamaMLP(nn.Module):\n",
    "    def __init__(self, config, torch_dtype=torch.float32):\n",
    "        super(LLamaMLP, self).__init__()\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        self.intermediate_size = config['intermediate_size']\n",
    "\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config['mlp_bias'], dtype=torch_dtype)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config['mlp_bias'], dtype=torch_dtype)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config['mlp_bias'], dtype=torch_dtype)\n",
    "\n",
    "        self.act_fn = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, torch_dtype, eps=1e-6):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size, dtype=torch_dtype))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_dtype = x.dtype\n",
    "        hidden_states = x.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        return self.weight * hidden_states.to(input_dtype)\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, layer_idx, config, torch_dtype=torch.float32):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.layer_idx = layer_idx\n",
    "        self.input_layernorm = RMSNorm(config['hidden_size'], torch_dtype, eps=config['rms_norm_eps'])\n",
    "\n",
    "        self.self_attn = SdpaAttention(layer_idx, config, torch_dtype=torch_dtype)\n",
    "\n",
    "        self.post_attention_layernorm = RMSNorm(config['hidden_size'], torch_dtype, eps=config['rms_norm_eps'])\n",
    "        self.mlp = LLamaMLP(config, torch_dtype)\n",
    "\n",
    "    def forward(self, x, position_ids, kv_cache=None):\n",
    "        residual = x\n",
    "        hidden_states = self.input_layernorm(x)\n",
    "        attention_output = self.self_attn(hidden_states, position_ids, kv_cache=kv_cache)\n",
    "        hidden_states = residual + attention_output\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = residual + self.mlp(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class LLama(nn.Module):\n",
    "    def __init__(self, config, torch_dtype=torch.float32):\n",
    "        super(LLama, self).__init__()\n",
    "        self.padding_idx = config['eos_token_id']\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config['vocab_size'], config['hidden_size'], padding_idx=self.padding_idx,\n",
    "                                         dtype=torch_dtype)\n",
    "        \n",
    "        self.layers = nn.ModuleList([DecoderLayer(layer_idx, config, torch_dtype) for layer_idx in range(config['num_hidden_layers'])])\n",
    "\n",
    "        self.norm = RMSNorm(config['hidden_size'], torch_dtype, eps=config['rms_norm_eps'])\n",
    "\n",
    "    def forward(self, x, position_ids=None, kv_cache=None):\n",
    "        \n",
    "        hidden_states = self.embed_tokens(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states, position_ids, kv_cache=kv_cache)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class LLamaGenerator(nn.Module):\n",
    "    def __init__(self, config, torch_dtype=torch.float32):\n",
    "        super(LLamaGenerator, self).__init__()\n",
    "        self.config = config\n",
    "        self.model = LLama(config, torch_dtype=torch_dtype)\n",
    "        self.lm_head = nn.Linear(config['hidden_size'], config['vocab_size'], dtype=torch_dtype, bias=False)\n",
    "\n",
    "    def forward(self, x, max_length=10, stop_token=None):\n",
    "        inp = x\n",
    "        fo = []\n",
    "        kv_cache = [None] * self.config['num_hidden_layers']\n",
    "        position_ids = torch.arange(inp.size(1)).unsqueeze(0).expand(x.size(0), -1)\n",
    "        with torch.no_grad():\n",
    "            for _ in tqdm(range(max_length)):\n",
    "                out = self.model(inp, position_ids=position_ids, kv_cache=kv_cache)\n",
    "                out = self.lm_head(out)[:, -1, :]\n",
    "                out = out.argmax(-1).unsqueeze(-1)\n",
    "                fo.append(out.item())\n",
    "                if stop_token is not None and out.item() == stop_token:\n",
    "                    break\n",
    "                inp = out\n",
    "                position_ids = torch.unsqueeze(position_ids[:, -1] + 1, 0)\n",
    "\n",
    "        return fo\n",
    "\n",
    "with init_empty_weights():\n",
    "    generator = LLamaGenerator(config, torch_dtype=torch.bfloat16)\n",
    "    \n",
    "generator.load_state_dict(state_dict, assign=True)\n",
    "generator.eval()\n",
    "\n"
   ],
   "id": "ca8d157fdddfaea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLamaGenerator(\n",
       "  (model): LLama(\n",
       "    (embed_tokens): Embedding(128256, 4096, padding_idx=128009)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x DecoderLayer(\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (self_attn): SdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (mlp): LLamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:58:34.187001Z",
     "start_time": "2024-06-11T13:54:46.017079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "out = generator(input_ids)\n",
    "\n",
    "print(out)\n",
    "tokenizer.decode(out)"
   ],
   "id": "2a55bd3ad9415728",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "92afe7b9870c488cb389a3346f86a4b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 11670, 24790, 10368, 2268, 38501, 4428, 25, 14098, 128009]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A classic classification exercise!\\n\\ntomato: fruit<|eot_id|>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T13:58:34.190727Z",
     "start_time": "2024-06-11T13:58:34.188443Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "92b1ed74c1fb4329",
   "outputs": [],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
