{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-10T14:44:44.994322Z",
     "start_time": "2024-06-10T14:44:41.152563Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from safetensors.torch import load_file\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import math\n",
    "from accelerate import init_empty_weights\n",
    "import gc"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T14:44:45.606687Z",
     "start_time": "2024-06-10T14:44:44.996022Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "shards = ['model-00001-of-00004.safetensors', 'model-00002-of-00004.safetensors', 'model-00003-of-00004.safetensors',\n",
    "          'model-00004-of-00004.safetensors']\n",
    "\n",
    "base_path = \"/users/melodi/gsantoss/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B-Instruct/snapshots/e1945c40cd546c78e41f1151f4db032b271faeaa/\"\n",
    "\n",
    "state_dict = {}\n",
    "for shard in shards:\n",
    "    state_dict.update(load_file(base_path + shard))\n",
    "\n",
    "with open(base_path + \"config.json\") as f:\n",
    "    config = json.load(f)"
   ],
   "id": "6f5f4c7aea46c78e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T14:44:45.611214Z",
     "start_time": "2024-06-10T14:44:45.607965Z"
    }
   },
   "cell_type": "code",
   "source": "config['mlp_bias'] = False",
   "id": "8236189972cce71b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T14:44:45.628051Z",
     "start_time": "2024-06-10T14:44:45.613085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for k, v in config.items():\n",
    "    print(k, v)"
   ],
   "id": "cb7d6753d672fb0e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architectures ['LlamaForCausalLM']\n",
      "attention_bias False\n",
      "attention_dropout 0.0\n",
      "bos_token_id 128000\n",
      "eos_token_id 128009\n",
      "hidden_act silu\n",
      "hidden_size 4096\n",
      "initializer_range 0.02\n",
      "intermediate_size 14336\n",
      "max_position_embeddings 8192\n",
      "model_type llama\n",
      "num_attention_heads 32\n",
      "num_hidden_layers 32\n",
      "num_key_value_heads 8\n",
      "pretraining_tp 1\n",
      "rms_norm_eps 1e-05\n",
      "rope_scaling None\n",
      "rope_theta 500000.0\n",
      "tie_word_embeddings False\n",
      "torch_dtype bfloat16\n",
      "transformers_version 4.40.0.dev0\n",
      "use_cache True\n",
      "vocab_size 128256\n",
      "mlp_bias False\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T14:44:45.830275Z",
     "start_time": "2024-06-10T14:44:45.629606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "base_ont = '/projets/melodi/gsantoss/data/oaei/tracks/populated/data_100'\n",
    "\n",
    "with open(f'{base_ont}/cmt_100.ttl') as f:\n",
    "    o1 = f.read()\n",
    "\n",
    "with open(f'{base_ont}/conference_100.ttl') as f:\n",
    "    o2 = f.read()\n",
    "\n",
    "txt = f'''\n",
    "Given the two ontologies bellow:\n",
    "\n",
    "<ontology1>\n",
    "{o1}    \n",
    "</ontology1>    \n",
    "<ontology2>\n",
    "{o2}\n",
    "</ontology2>\n",
    "\n",
    "And one example of alignment between two different ontologies:\n",
    "\n",
    "<ontology1>\n",
    "@prefix lib: <http://example.org/library#> .\n",
    "@prefix dcterms: <http://purl.org/dc/terms/> .\n",
    "@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n",
    "\n",
    "lib:Book1 a lib:Book ;\n",
    "    dcterms:title \"The Catcher in the Rye\" ;\n",
    "    dcterms:creator lib:Author1 ;\n",
    "    lib:hasGenre \"Fiction\" .\n",
    "\n",
    "lib:Author1 a lib:Author ;\n",
    "    foaf:name \"J.D. Salinger\" ;\n",
    "    foaf:birthDate \"1919-01-01\" .\n",
    "</ontology1>\n",
    "<ontology2>\n",
    "@prefix pub: <http://example.org/publishing#> .\n",
    "@prefix dcterms: <http://purl.org/dc/terms/> .\n",
    "@prefix foaf: <http://xmlns.com/foaf/0.1/> .\n",
    "\n",
    "pub:Book1 a pub:Book ;\n",
    "    dcterms:title \"To Kill a Mockingbird\" ;\n",
    "    dcterms:creator pub:Author1 ;\n",
    "    pub:publicationYear \"1960\" .\n",
    "\n",
    "pub:Author1 a pub:Author ;\n",
    "    foaf:name \"Harper Lee\" ;\n",
    "    pub:hasNationality \"American\" .\n",
    "</ontology2>\n",
    "<alignment>\n",
    "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
    "<rdf:RDF xmlns=\"http://knowledgeweb.semanticweb.org/heterogeneity/alignment\"\n",
    "         xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n",
    "         xmlns:xsd=\"http://www.w3.org/2001/XMLSchema#\"\n",
    "         xmlns:align=\"http://knowledgeweb.semanticweb.org/heterogeneity/alignment#\"\n",
    "         xmlns:edoal=\"http://ns.inria.org/edoal/1.0/#\">\n",
    "\n",
    "  <Alignment>\n",
    "    <xml>yes</xml>\n",
    "    <level>2EDOAL</level>\n",
    "    <type>**</type>\n",
    "    \n",
    "    <onto1>\n",
    "      <Ontology rdf:about=\"http://example.org/library#\"/>\n",
    "    </onto1>\n",
    "    <onto2>\n",
    "      <Ontology rdf:about=\"http://example.org/publishing#\"/>\n",
    "    </onto2>\n",
    "\n",
    "    <map>\n",
    "      <Cell>\n",
    "        <entity1 rdf:resource=\"http://example.org/library#Book\"/>\n",
    "        <entity2 rdf:resource=\"http://example.org/publishing#Book\"/>\n",
    "        <relation>=</relation>\n",
    "        <measure>1.0</measure>\n",
    "      </Cell>\n",
    "    </map>\n",
    "    <map>\n",
    "      <Cell>\n",
    "        <entity1 rdf:resource=\"http://example.org/library#Author\"/>\n",
    "        <entity2 rdf:resource=\"http://example.org/publishing#Author\"/>\n",
    "        <relation>=</relation>\n",
    "        <measure>1.0</measure>\n",
    "      </Cell>\n",
    "    </map>\n",
    "  </Alignment>\n",
    "</rdf:RDF>\n",
    "</alignment>\n",
    "\n",
    "Write a file in EDOAL format containing the complex alignment between the ontology1 and ontology2. You don't need to explain yourself. Just give as response the resulting file without saying anything. Here is one example bellow:\n",
    "'''"
   ],
   "id": "42a5292dea3be099",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T14:45:22.452486Z",
     "start_time": "2024-06-10T14:44:45.831605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_prompt = '''apple: fruit\n",
    "orange: fruit\n",
    "zucchini: vegetable\n",
    "tomato:\n",
    "\n",
    "Complete this list'''\n",
    "\n",
    "messages = [\n",
    "    # {\"role\": \"system\", \"content\": \"You are an Ontology Alignment expert. You are able to align two ontologies by creating a file in EDOAL format containing the result alignments. You are able to produce complex alignments that are those involving multiple entities and relationships in a n:m cardinality. The user will provide you with two ontologies and you respond with the EDOAL file containing the alignments. You don't need to explain yourself. Just give as response the resulting file without saying anything.\"},\n",
    "    {\"role\": \"user\", \"content\": txt},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(input_ids.shape)"
   ],
   "id": "e05a856a4042c1b6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8066417])\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T14:45:22.620493Z",
     "start_time": "2024-06-10T14:45:22.495474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Cache:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n",
    "        super().__init__()\n",
    "        self.scaling_factor = scaling_factor\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        self.max_seq_len_cached = max_position_embeddings\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "\n",
    "        device_type = x.device.type\n",
    "        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n",
    "        with torch.autocast(device_type=device_type, enabled=False):\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            cos = emb.cos()\n",
    "            sin = emb.sin()\n",
    "        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "\n",
    "class SdpaAttention(nn.Module):\n",
    "    def __init__(self, layer_idx, config, torch_dtype=torch.float32):\n",
    "        super(SdpaAttention, self).__init__()\n",
    "        self.layer_idx = layer_idx\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        self.num_heads = config['num_attention_heads']\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        self.num_key_value_heads = config['num_key_value_heads']\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "\n",
    "    \n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config['attention_bias'],\n",
    "                                dtype=torch_dtype)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim,\n",
    "                                bias=config['attention_bias'], dtype=torch_dtype)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim,\n",
    "                                bias=config['attention_bias'], dtype=torch_dtype)\n",
    "        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config['attention_bias'], dtype=torch_dtype)\n",
    "        self.max_position_embeddings = config['max_position_embeddings']\n",
    "        self.rope_theta = config['rope_theta']\n",
    "        self.rotary_emb = RotaryEmbedding(self.head_dim, max_position_embeddings=self.max_position_embeddings, base=self.rope_theta)\n",
    "        self.attention_dropout = config['attention_dropout']\n",
    "\n",
    "    def forward(self, x, position_ids, kv_cache=None):\n",
    "        \n",
    "        \n",
    "        # print('qkv')\n",
    "        # self.q_proj.cuda(0)\n",
    "        # self.k_proj.cuda(0)\n",
    "        # self.v_proj.cuda(0)\n",
    "        # \n",
    "        # for i, s in enumerate(torch.split(x, 1_000_000, 1)):\n",
    "        #     lx = torch.load(f'/projets/melodi/gsantoss/tmp/tmpe/ie_{i}.pt')\n",
    "        #     print(lx.shape)\n",
    "        #     tq = []\n",
    "        # \n",
    "        #     for sm in torch.chunk(lx, 3, 1):\n",
    "        #         tq.append(self.q_proj(sm.cuda(0)).cpu())\n",
    "        # \n",
    "        #     tq = torch.cat(tq, 1)\n",
    "        #     bsz, q_len, _ = tq.size()\n",
    "        #     tq = tq.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # \n",
    "        #     \n",
    "        #     for j, small_part in enumerate(torch.split(tq, 500_000, 2)):\n",
    "        #         torch.save(small_part, f'/projets/melodi/gsantoss/tmp/tmpe/qie_{2*i + j}.pt')\n",
    "        #         \n",
    "        #     tq = []\n",
    "        # \n",
    "        #     for sm in torch.chunk(lx, 3, 1):\n",
    "        #         tq.append(self.k_proj(sm.cuda(0)).cpu())\n",
    "        # \n",
    "        #     tq = torch.cat(tq, 1)\n",
    "        # \n",
    "        #     bsz, q_len, _ = tq.size()\n",
    "        #     tq = tq.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        #     \n",
    "        #     for j, small_part in enumerate(torch.split(tq, 500_000, 2)):\n",
    "        #         torch.save(small_part, f'/projets/melodi/gsantoss/tmp/tmpe/kie_{2*i + j}.pt')\n",
    "        #     \n",
    "        #     tq = []\n",
    "        # \n",
    "        #     for sm in torch.chunk(lx, 3, 1):\n",
    "        #         tq.append(self.v_proj(sm.cuda(0)).cpu())\n",
    "        # \n",
    "        #     tq = torch.cat(tq, 1)\n",
    "        # \n",
    "        #     bsz, q_len, _ = tq.size()\n",
    "        #     tq = tq.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        #     \n",
    "        #     for j, small_part in enumerate(torch.split(tq, 500_000, 2)):\n",
    "        #         torch.save(small_part, f'/projets/melodi/gsantoss/tmp/tmpe/vie_{2*i + j}.pt')\n",
    "        # \n",
    "        #     gc.collect()\n",
    "        #     torch.cuda.empty_cache()\n",
    "        # \n",
    "        # \n",
    "        # gc.collect()\n",
    "        # torch.cuda.empty_cache()\n",
    "        # \n",
    "        # print('finish qkv')\n",
    "        \n",
    "        \n",
    "        # query_states = self.q_proj(x)\n",
    "        # key_states = self.k_proj(x)\n",
    "        # value_states = self.v_proj(x)\n",
    "        \n",
    "        # for i, s in enumerate(torch.split(x, 1_000_000, 1)):\n",
    "            \n",
    "        # query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        # value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        \n",
    "        # print('rotary')\n",
    "        # self.rotary_emb.cuda(0)\n",
    "        # \n",
    "        # tcos = []\n",
    "        # tsin = []\n",
    "        # \n",
    "        # for i, (s, p) in enumerate(zip(torch.split(x, 500_000, 1), torch.split(position_ids, 500_000, 1))):\n",
    "        #     lx = torch.load(f'/projets/melodi/gsantoss/tmp/tmpe/vie_{i}.pt')\n",
    "        # \n",
    "        #     cos, sin = self.rotary_emb(lx.cuda(0), p.cuda(0))\n",
    "        #     tcos.append(cos.cpu())\n",
    "        #     tsin.append(sin.cpu())\n",
    "        # \n",
    "        # cos = torch.cat(tcos, 1)\n",
    "        # sin = torch.cat(tsin, 1)\n",
    "        # \n",
    "        # tcos = None\n",
    "        # tsin = None\n",
    "        # gc.collect()\n",
    "        # \n",
    "        # for i, (splt, c, s) in enumerate(zip(torch.split(x, 500_000, 1), torch.split(cos, 500_000, 1), torch.split(sin, 500_000, 1))):\n",
    "        #     qs = torch.load(f'/projets/melodi/gsantoss/tmp/tmpe/qie_{i}.pt')\n",
    "        #     ks = torch.load(f'/projets/melodi/gsantoss/tmp/tmpe/kie_{i}.pt')\n",
    "        # \n",
    "        #     tq = []\n",
    "        #     tk = []\n",
    "        #     split_len = 50_000\n",
    "        # \n",
    "        #     for sqs, sks, sc, ss in zip(torch.split(qs, split_len, 2), torch.split(ks, split_len, 2), torch.split(c, split_len, 1), torch.split(s, split_len, 1)):\n",
    "        # \n",
    "        #         with torch.device('cuda:0'):\n",
    "        #             query_states, key_states = apply_rotary_pos_emb(sqs.cuda(0), sks.cuda(0), sc.cuda(0), ss.cuda(0))\n",
    "        #         tq.append(query_states.cpu())\n",
    "        #         tk.append(key_states.cpu())\n",
    "        #         gc.collect()\n",
    "        #         torch.cuda.empty_cache()\n",
    "        # \n",
    "        #     qs = None\n",
    "        #     ks = None\n",
    "        #     gc.collect()\n",
    "        #     torch.cuda.empty_cache()\n",
    "        #     tq = torch.cat(tq, 2)\n",
    "        #     print(tq.shape)\n",
    "        #     torch.save(tq, f'/projets/melodi/gsantoss/tmp/tmpe/fqie_{i}.pt')\n",
    "        #     torch.save(torch.cat(tk, 2), f'/projets/melodi/gsantoss/tmp/tmpe/fkie_{i}.pt')\n",
    "        #     tq = None\n",
    "        #     tk = None\n",
    "        #     gc.collect()\n",
    "\n",
    "        \n",
    "        \n",
    "        # if kv_cache is not None:\n",
    "        #     if kv_cache[self.layer_idx] is not None:\n",
    "        #         past_key, past_value = kv_cache[self.layer_idx]\n",
    "        #         key_states = torch.cat([past_key, key_states], dim=2)\n",
    "        #         value_states = torch.cat([past_value, value_states], dim=2)\n",
    "        #         \n",
    "        #         \n",
    "        #         \n",
    "        #         \n",
    "        #     for i, s in enumerate(torch.split(x, 500_000, 1)):\n",
    "        #         kie = torch.load(f'/projets/melodi/gsantoss/tmp/tmpe/fkie_{i}.pt')\n",
    "        #         torch.save(kie, f'/projets/melodi/gsantoss/tmp/tmpe/cache_{self.layer_idx}_kie_{i}.pt')\n",
    "        #         \n",
    "        #         kie = torch.load(f'/projets/melodi/gsantoss/tmp/tmpe/vie_{i}.pt')\n",
    "        #         torch.save(kie, f'/projets/melodi/gsantoss/tmp/tmpe/cache_{self.layer_idx}_vie_{i}.pt')\n",
    "        #         print(kie.shape)\n",
    "        #         \n",
    "        #     kv_cache[self.layer_idx] = True\n",
    "        \n",
    "        \n",
    "        # key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "\n",
    "        \n",
    "        \n",
    "        for i, s in enumerate(torch.split(x, 500_000, 1)):\n",
    "            query_states = torch.load(f'/projets/melodi/gsantoss/tmp/tmpe/qie_{i}.pt')\n",
    "            print(query_states.shape)\n",
    "        \n",
    "        # for i, s in enumerate(torch.split(x, 500_000, 1)):\n",
    "        #     key_states = torch.load(f'/projets/melodi/gsantoss/tmp/tmpe/cache_{self.layer_idx}_kie_{i}.pt')\n",
    "        #     print(key_states.shape)\n",
    "        #     key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        #     print(key_states.shape)\n",
    "        #     raise Exception('hue')\n",
    "            \n",
    "        raise Exception('hue')\n",
    "        \n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        causal_mask = torch.triu(torch.full((q_len, q_len), -1e9), diagonal=1)\n",
    "        attn_weights = attn_weights + causal_mask\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n",
    "        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
    "        \n",
    "        \n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.view(bsz, q_len, -1)\n",
    "\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        return attn_output\n",
    "\n",
    "\n",
    "class LLamaMLP(nn.Module):\n",
    "    def __init__(self, config, torch_dtype=torch.float32):\n",
    "        super(LLamaMLP, self).__init__()\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        self.intermediate_size = config['intermediate_size']\n",
    "\n",
    "        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config['mlp_bias'], dtype=torch_dtype)\n",
    "        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config['mlp_bias'], dtype=torch_dtype)\n",
    "        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config['mlp_bias'], dtype=torch_dtype)\n",
    "\n",
    "        self.act_fn = nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, torch_dtype, eps=1e-6):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size, dtype=torch_dtype))\n",
    "        self.variance_epsilon = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_dtype = x.dtype\n",
    "        hidden_states = x.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "        \n",
    "        return self.weight * hidden_states.to(input_dtype)\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, layer_idx, config, torch_dtype=torch.float32):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.layer_idx = layer_idx\n",
    "        self.input_layernorm = RMSNorm(config['hidden_size'], torch_dtype, eps=config['rms_norm_eps'])\n",
    "\n",
    "        self.self_attn = SdpaAttention(layer_idx, config, torch_dtype=torch_dtype)\n",
    "\n",
    "        self.post_attention_layernorm = RMSNorm(config['hidden_size'], torch_dtype, eps=config['rms_norm_eps'])\n",
    "        self.mlp = LLamaMLP(config, torch_dtype)\n",
    "\n",
    "    def forward(self, x, position_ids, kv_cache=None):\n",
    "        residual = x\n",
    "        hidden_states = x\n",
    "        # with torch.device('meta'):\n",
    "        #     hidden_states = self.input_layernorm(x.clone().to('meta'))\n",
    "        \n",
    "        # for i, s in enumerate(torch.split(x, 1_000_000, 1)):\n",
    "        #     re = torch.load(f'/projets/melodi/gsantoss/tmp/tmpe/ie_{i}.pt')\n",
    "        # \n",
    "        #     nre1 = self.input_layernorm(re[:, :re.shape[1] // 2, :])\n",
    "        #     nre2 = self.input_layernorm(re[:, re.shape[1] // 2 :, :])\n",
    "        #     re = None\n",
    "        #     gc.collect()\n",
    "        #     out = torch.cat([nre1, nre2], dim=1)\n",
    "        #     \n",
    "        #     nre1 = None\n",
    "        #     nre2 = None\n",
    "        #     print(out.shape)\n",
    "        #     gc.collect()\n",
    "        #     torch.save(out, f'/projets/melodi/gsantoss/tmp/tmpe/ie_{i}.pt')\n",
    "            \n",
    "        \n",
    "        attention_output = self.self_attn(hidden_states, position_ids, kv_cache=kv_cache)\n",
    "        hidden_states = residual + attention_output\n",
    "\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = residual + self.mlp(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class LLama(nn.Module):\n",
    "    def __init__(self, config, torch_dtype=torch.float32):\n",
    "        super(LLama, self).__init__()\n",
    "        self.d_type = torch_dtype\n",
    "        self.padding_idx = config['eos_token_id']\n",
    "        self.config = config\n",
    "        self.embed_tokens = nn.Embedding(config['vocab_size'], config['hidden_size'], padding_idx=self.padding_idx,\n",
    "                                         dtype=torch_dtype)\n",
    "        \n",
    "        self.layers = nn.ModuleList([DecoderLayer(layer_idx, config, torch_dtype) for layer_idx in range(config['num_hidden_layers'])])\n",
    "\n",
    "        self.norm = RMSNorm(config['hidden_size'], torch_dtype, eps=config['rms_norm_eps'])\n",
    "\n",
    "    def forward(self, x, position_ids=None, kv_cache=None):\n",
    "        \n",
    "        with torch.device('meta'):\n",
    "            hidden_states = self.embed_tokens(x.clone().to('meta'))\n",
    "        \n",
    "        # for i, s in enumerate(torch.split(x, 1_000_000, 1)):\n",
    "        #     emb = self.embed_tokens(s)\n",
    "        #     print(emb.shape)\n",
    "        #     torch.save(emb, f'/projets/melodi/gsantoss/tmp/tmpe/ie_{i}.pt')\n",
    "        # \n",
    "        # gc.collect()\n",
    "        # print(hidden_states.shape, hidden_states.device)\n",
    "        # hidden_states = self.embed_tokens(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            hidden_states = layer(hidden_states, position_ids, kv_cache=kv_cache)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class LLamaGenerator(nn.Module):\n",
    "    def __init__(self, config, torch_dtype=torch.float32):\n",
    "        super(LLamaGenerator, self).__init__()\n",
    "        self.config = config\n",
    "        self.model = LLama(config, torch_dtype=torch_dtype)\n",
    "        self.lm_head = nn.Linear(config['hidden_size'], config['vocab_size'], dtype=torch_dtype, bias=False)\n",
    "\n",
    "    def forward(self, x, max_length=10, stop_token=None):\n",
    "        inp = x\n",
    "        fo = []\n",
    "        kv_cache = [None] * self.config['num_hidden_layers']\n",
    "        position_ids = torch.arange(inp.size(1)).unsqueeze(0).expand(x.size(0), -1)\n",
    "        with torch.no_grad():\n",
    "            for _ in tqdm(range(max_length)):\n",
    "                out = self.model(inp, position_ids=position_ids, kv_cache=kv_cache)\n",
    "                out = self.lm_head(out)[:, -1, :]\n",
    "                out = out.argmax(-1).unsqueeze(-1)\n",
    "                fo.append(out.item())\n",
    "                if stop_token is not None and out.item() == stop_token:\n",
    "                    break\n",
    "                inp = out\n",
    "                position_ids = torch.unsqueeze(position_ids[:, -1] + 1, 0)\n",
    "\n",
    "        return fo\n",
    "\n",
    "with init_empty_weights():\n",
    "    generator = LLamaGenerator(config, torch_dtype=torch.bfloat16)\n",
    "    \n",
    "generator.load_state_dict(state_dict, assign=True)\n",
    "generator.eval()\n",
    "\n"
   ],
   "id": "ca8d157fdddfaea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLamaGenerator(\n",
       "  (model): LLama(\n",
       "    (embed_tokens): Embedding(128256, 4096, padding_idx=128009)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x DecoderLayer(\n",
       "        (input_layernorm): RMSNorm()\n",
       "        (self_attn): SdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "        )\n",
       "        (post_attention_layernorm): RMSNorm()\n",
       "        (mlp): LLamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-10T14:45:22.621798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MetadataTensor(object):\n",
    "    def __init__(self, data, metadata=None, **kwargs):\n",
    "        self._t = torch.as_tensor(data, **kwargs)\n",
    "        self._metadata = metadata\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Metadata:\\n{}\\n\\ndata:\\n{}\".format(self._metadata, self._t)\n",
    "\n",
    "    @classmethod\n",
    "    def __torch_function__(cls, func, types, args=(), kwargs=None):\n",
    "        if kwargs is None:\n",
    "            kwargs = {}\n",
    "        metadatas = tuple(a._metadata for a in args if hasattr(a, '_metadata'))\n",
    "        args = [getattr(a, '_t', a) for a in args]\n",
    "        assert len(metadatas) > 0\n",
    "        ret = func(*args, **kwargs)\n",
    "        return MetadataTensor(ret, metadata=metadatas[0])\n",
    "\n",
    "\n",
    "out = generator(input_ids, max_length=10, stop_token=tokenizer.eos_token_id)\n",
    "\n",
    "print(out)\n",
    "tokenizer.decode(out)"
   ],
   "id": "2a55bd3ad9415728",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c69ce92b440241049c2c6e03a9ed9fca"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 500000, 128])\n",
      "torch.Size([1, 32, 500000, 128])\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6df217b816e84d6b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
