{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-29T10:16:58.134266Z",
     "start_time": "2024-07-29T10:16:48.288770Z"
    }
   },
   "source": [
    "from rdflib import Graph\n",
    "from rdflib.term import URIRef, Literal, BNode\n",
    "from rdflib.namespace import RDF, RDFS, OWL, XSD\n",
    "import itertools\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "from torch import nn\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "from rag_reduce import ont_query_reduce\n",
    "import gc"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T15:12:06.570731Z",
     "start_time": "2024-07-15T15:12:05.461447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "# model_id = \"google/gemma-2b-it\"\n",
    "# model_id = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "# model_id = \"unsloth/llama-3-8b-bnb-4bit\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ],
   "id": "c76332563870249c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T15:13:42.209997Z",
     "start_time": "2024-07-15T15:13:42.198839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "txt = gen_prompt(r1, r2, query, include_sample1=True, include_sample2=True)\n",
    "\n",
    "messages = [\n",
    "    # {\"role\": \"system\", \"content\": \"You are an Ontology Alignment expert. You are able to align two ontologies by creating a file in EDOAL format containing the result alignments. You are able to produce complex alignments that are those involving multiple entities and relationships in a n:m cardinality. The user will provide you with two ontologies and you respond with the EDOAL file containing the alignments. You don't need to explain yourself. Just give as response the resulting file without saying anything.\"},\n",
    "    {\"role\": \"user\", \"content\": txt},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(input_ids.shape)"
   ],
   "id": "da9aa9ad848ca529",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3325])\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T15:14:23.114161Z",
     "start_time": "2024-07-15T15:13:53.504637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# model_id = \"google/gemma-2b-it\"\n",
    "# model_id = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "# model_id = \"unsloth/llama-3-8b-bnb-4bit\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map='auto',\n",
    "    quantization_config=quantization_config,\n",
    "    low_cpu_mem_usage=True\n",
    "\n",
    ")"
   ],
   "id": "e2f4fea720395fbe",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32e4e186f1864ed29fec2c5eefec1c97"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T15:14:24.696547Z",
     "start_time": "2024-07-15T15:14:24.679887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "messages = [\n",
    "    # {\"role\": \"system\", \"content\": \"You are an Ontology Alignment expert. You are able to align two ontologies by creating a file in EDOAL format containing the result alignments. You are able to produce complex alignments that are those involving multiple entities and relationships in a n:m cardinality. The user will provide you with two ontologies and you respond with the EDOAL file containing the alignments. You don't need to explain yourself. Just give as response the resulting file without saying anything.\"},\n",
    "    {\"role\": \"user\", \"content\": txt},\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n",
    "\n",
    "print(input_ids.shape)"
   ],
   "id": "7ecabbe436f9fc29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3325])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T15:15:09.882537Z",
     "start_time": "2024-07-15T15:14:27.035053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=2 * 1024,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=False,\n",
    "        temperature=None,\n",
    "        top_p=None,\n",
    "\n",
    "    )\n",
    "response = outputs[0][input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))"
   ],
   "id": "783cd9236dd55d12",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `None` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
      "<rdf:RDF xmlns=\"http://knowledgeweb.semanticweb.org/heterogeneity/alignment\"\n",
      "         xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"\n",
      "         xmlns:xsd=\"http://www.w3.org/2001/XMLSchema#\"\n",
      "         xmlns:align=\"http://knowledgeweb.semanticweb.org/heterogeneity/alignment#\"\n",
      "         xmlns:edoal=\"http://ns.inria.org/edoal/1.0/#\">\n",
      "    <Alignment>\n",
      "        <xml>yes</xml>\n",
      "        <level>2EDOAL</level>\n",
      "        <type>**</type>\n",
      "        <onto1>\n",
      "            <Ontology rdf:about=\"http://cmt#\"/>\n",
      "        </onto1>\n",
      "        <onto2>\n",
      "            <Ontology rdf:about=\"http://confOf#\"/>\n",
      "        </onto2>\n",
      "        <map>\n",
      "            <Cell>\n",
      "                <entity1 rdf:resource=\"http://cmt#Administrator\"/>\n",
      "                <entity2>\n",
      "                    <edoal:Class>\n",
      "                        <edoal:And>\n",
      "                            <edoal:Class rdf:about=\"http://confOf#Administrator\"/>\n",
      "                            <edoal:PropertyRestriction>\n",
      "                                <edoal:onProperty rdf:resource=\"http://confOf#hasAdministrativeEvent\"/>\n",
      "                                <edoal:someValuesFrom rdf:resource=\"http://confOf#Administrative_event\"/>\n",
      "                            </edoal:PropertyRestriction>\n",
      "                        </edoal:And>\n",
      "                    </edoal:Class>\n",
      "                </entity2>\n",
      "                <relation>=</relation>\n",
      "                <measure>1.0</measure>\n",
      "            </Cell>\n",
      "        </map>\n",
      "    </Alignment>\n",
      "</rdf:RDF>\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3bb2a55f97205bf9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
